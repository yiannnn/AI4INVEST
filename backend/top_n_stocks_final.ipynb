{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "full_pipeline.py\n",
        "1) Load tickers & risk buckets CSV.\n",
        "\n",
        "2) Retrieve End of Day(Stooq) price data for the listed tickers along with SPY.\n",
        "\n",
        "3) Create sliding-window samples for volume 30, momentum, 30 day beta, and future return.\n",
        "\n",
        "4) Median impute and scale to standard metrics.\n",
        "\n",
        "5) Split in train/test + 5-fold CV and report MSE, then retrain on full data.\n",
        "\n",
        "6) Export the imputer, scaler and Random Forest model as joblib.\n",
        "\n",
        "7) Predict on each ticker’s most recent features ensuring TOP_N selections by bucket with ordinal rank (positives first).\n",
        "\n",
        "8) Export the predictions as CSV with standardized column names.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas_datareader import data as pdr\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import joblib\n",
        "\n",
        "# PARAMETERS\n",
        "INPUT_FILE     = \"stock_risk_kmeans_robust.csv\"   # columns: index=ticker, risk_label2\n",
        "LOOKAHEAD_DAYS = 90\n",
        "MIN_HIST_DAYS  = 60\n",
        "TOP_N          = 5\n",
        "SLEEP_SEC      = 1\n",
        "OUTPUT_FILE    = \"top_n_per_category.csv\"\n",
        "RANDOM_STATE   = 42\n",
        "\n",
        "\n",
        "def fetch_close(symbol: str) -> pd.Series:\n",
        "    \"\"\"Fetch EOD Close prices via Stooq.\"\"\"\n",
        "    try:\n",
        "        df = pdr.DataReader(symbol, \"stooq\")\n",
        "        return df.sort_index()[\"Close\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Fetch failed for {symbol}: {e}\")\n",
        "        return pd.Series(dtype=float)\n",
        "\n",
        "\n",
        "def build_feature_df(df0: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Generate sliding-window features and target returns.\"\"\"\n",
        "    tickers = df0.index.tolist()\n",
        "    spy = fetch_close(\"SPY\").pct_change().dropna().rename(\"SPY\")\n",
        "    time.sleep(SLEEP_SEC)\n",
        "\n",
        "    price_hist = {}\n",
        "    for sym in tickers:\n",
        "        series = fetch_close(sym).sort_index()\n",
        "        if len(series) > MIN_HIST_DAYS + LOOKAHEAD_DAYS:\n",
        "            price_hist[sym] = series\n",
        "        else:\n",
        "            print(f\"{sym}: insufficient history, skipping\")\n",
        "        time.sleep(SLEEP_SEC)\n",
        "\n",
        "    rows = []\n",
        "    for sym, price in price_hist.items():\n",
        "        ret = price.pct_change().dropna()\n",
        "        n = len(price)\n",
        "        start = max(30, MIN_HIST_DAYS)\n",
        "        end = n - LOOKAHEAD_DAYS\n",
        "        for i in range(start, end):\n",
        "            vol30 = ret.iloc[i-29:i+1].std()\n",
        "            mom30 = price.iloc[i] / price.iloc[i-30] - 1\n",
        "            if i >= MIN_HIST_DAYS:\n",
        "                spy_a = spy.reindex(ret.index)\n",
        "                w_ret = ret.iloc[i-59:i+1].values\n",
        "                w_spy = spy_a.iloc[i-59:i+1].values\n",
        "                cov = np.cov(w_ret, w_spy, ddof=0)[0,1]\n",
        "                var = np.var(w_spy, ddof=0)\n",
        "                beta60 = cov/var if var > 0 else np.nan\n",
        "            else:\n",
        "                beta60 = np.nan\n",
        "            future_price = price.iloc[i + LOOKAHEAD_DAYS]\n",
        "            future_ret   = future_price / price.iloc[i] - 1\n",
        "            rows.append({\n",
        "                \"ticker\":        sym,\n",
        "                \"date\":          price.index[i],\n",
        "                \"vol30\":         vol30,\n",
        "                \"mom30\":         mom30,\n",
        "                \"beta60\":        beta60,\n",
        "                \"future_return\": future_ret,\n",
        "                \"risk_label2\":   df0.loc[sym, \"risk_label2\"]\n",
        "            })\n",
        "    feat_df = pd.DataFrame(rows).set_index([\"ticker\", \"date\"])\n",
        "    if feat_df.empty:\n",
        "        raise RuntimeError(\"No valid data to build features/targets!\")\n",
        "    return feat_df\n",
        "\n",
        "\n",
        "def preprocess_features(X: pd.DataFrame):\n",
        "    \"\"\"Impute missing values and standard scale features.\"\"\"\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    X_imp   = imputer.fit_transform(X)\n",
        "    scaler  = StandardScaler()\n",
        "    X_scaled= scaler.fit_transform(X_imp)\n",
        "    return X_scaled, imputer, scaler\n",
        "\n",
        "\n",
        "def train_and_evaluate(X: np.ndarray, y: pd.Series) -> RandomForestRegressor:\n",
        "    \"\"\"Train/test split, CV MSE report, then retrain on full data.\"\"\"\n",
        "    X_tr, X_ts, y_tr, y_ts = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
        "    )\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)\n",
        "    cv_scores = cross_val_score(\n",
        "        model, X_tr, y_tr, cv=5, scoring=\"neg_mean_squared_error\"\n",
        "    )\n",
        "    print(f\"CV MSE: {-cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "    model.fit(X_tr, y_tr)\n",
        "    test_mse = np.mean((model.predict(X_ts) - y_ts)**2)\n",
        "    print(f\"Test MSE: {test_mse:.4f}\")\n",
        "    final_model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)\n",
        "    final_model.fit(X, y)\n",
        "    return final_model\n",
        "\n",
        "\n",
        "def save_artifacts(imputer, scaler, model):\n",
        "    \"\"\"Save preprocessing and model artifacts.\"\"\"\n",
        "    joblib.dump(imputer, \"imputer.joblib\")\n",
        "    joblib.dump(scaler,  \"scaler.joblib\")\n",
        "    joblib.dump(model,   \"rf_model.joblib\")\n",
        "    print(\"Saved artifacts: imputer.joblib, scaler.joblib, rf_model.joblib\")\n",
        "\n",
        "\n",
        "def select_top_n(feat_df: pd.DataFrame, model, imputer, scaler, top_n: int = TOP_N) -> pd.DataFrame:\n",
        "    \"\"\"Predict returns, ensure TOP_N picks per bucket (positives first), and save.\"\"\"\n",
        "    latest = feat_df.groupby(level=0).tail(1).copy()\n",
        "    X_raw = latest[[\"vol30\",\"mom30\",\"beta60\"]]\n",
        "    X_imp = imputer.transform(X_raw)\n",
        "    X_scaled = scaler.transform(X_imp)\n",
        "    latest[\"pred_return\"] = model.predict(X_scaled)\n",
        "\n",
        "    # Re-map risk_label2 to Low/Medium/High by vol30 median\n",
        "    medians = latest.groupby(\"risk_label2\")[\"vol30\"].median().sort_values()\n",
        "    mapping = {orig: new for orig, new in zip(medians.index, [\"Low\",\"Medium\",\"High\"]) }\n",
        "    latest[\"risk_label\"] = latest[\"risk_label2\"].map(mapping)\n",
        "\n",
        "    picks = []\n",
        "    for label in [\"Low\",\"Medium\",\"High\"]:\n",
        "        bucket = latest[latest[\"risk_label\"] == label]\n",
        "        positive = bucket[bucket[\"pred_return\"] > 0]\n",
        "        top_pos = positive.nlargest(top_n, \"pred_return\")\n",
        "        if len(top_pos) < top_n:\n",
        "            remainder = bucket.drop(top_pos.index)\n",
        "            top_neg = remainder.nlargest(top_n - len(top_pos), \"pred_return\")\n",
        "            picks.append(pd.concat([top_pos, top_neg]))\n",
        "        else:\n",
        "            picks.append(top_pos)\n",
        "\n",
        "    final_df = pd.concat(picks)\n",
        "    final_df.to_csv(\n",
        "        OUTPUT_FILE,\n",
        "        columns=[\"vol30\",\"mom30\",\"beta60\",\"pred_return\",\"risk_label\"]\n",
        "    )\n",
        "    print(f\"Top {top_n} picks per bucket → '{OUTPUT_FILE}'\")\n",
        "    return final_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    df0     = pd.read_csv(INPUT_FILE, index_col=\"ticker\")\n",
        "    feat_df = build_feature_df(df0)\n",
        "    X = feat_df[[\"vol30\",\"mom30\",\"beta60\"]]\n",
        "    y = feat_df[\"future_return\"]\n",
        "    X_scaled, imp, scaler = preprocess_features(X)\n",
        "    model = train_and_evaluate(X_scaled, y)\n",
        "    save_artifacts(imp, scaler, model)\n",
        "    select_top_n(feat_df, model, imp, scaler)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwtQrx5u9Hio",
        "outputId": "3f61b3d1-3a93-41f6-e761-bcebb94f2aed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV MSE: 0.0509 ± 0.0012\n",
            "Test MSE: 0.0507\n",
            "Saved artifacts: imputer.joblib, scaler.joblib, rf_model.joblib\n",
            "Top 5 picks per bucket → 'top_n_per_category.csv'\n"
          ]
        }
      ]
    }
  ]
}